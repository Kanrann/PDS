import json
import os
import statistics
import re

# ================= é…ç½® =================
# ä½ çš„ç»“æœæ–‡ä»¶è·¯å¾„
FILE_PATH = r""

# ä½ å½“å‰è®¾ç½®çš„ç”Ÿæˆé™åˆ¶ (ç”¨äºè®¡ç®—æˆªæ–­é£é™©)
CURRENT_MAX_TOKENS = 1024 
# ç²—ç•¥æ¢ç®—ï¼š1 token â‰ˆ 1.5 ä¸­æ–‡å­—ç¬¦ (æ ¹æ® DeepSeek tokenizer ä¼°ç®—)
TOKEN_CHAR_RATIO = 1.5 
WARNING_LENGTH = CURRENT_MAX_TOKENS * TOKEN_CHAR_RATIO * 0.9 # è¾¾åˆ° 90% é•¿åº¦é¢„è­¦

def check_quality(text):
    """ç®€å•åˆ¤æ–­å•æ¡æ•°æ®çš„å«é‡‘é‡"""
    has_formula = 1 if ("$" in text or "\\" in text) else 0
    # é€»è¾‘è¯åº“
    logic_words = ["å½’å› äº", "å¯¼è‡´", "æ„å‘³ç€", "è¡¨æ˜", "æ¨å¯¼", "éµå¾ª", "å› æ­¤", "because", "due to"]
    has_logic = 1 if any(w in text for w in logic_words) else 0
    return has_formula, has_logic

def analyze_output_sweet_spot():
    print(f"æ­£åœ¨åˆ†æç»“æœæ–‡ä»¶: {FILE_PATH} ...")
    
    if not os.path.exists(FILE_PATH):
        print("âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œç”Ÿæˆè„šæœ¬ã€‚")
        return

    instruction_lens = [] # é—®é¢˜é•¿åº¦
    output_lens = []      # ç­”æ¡ˆé•¿åº¦
    high_quality_indices = [] # é«˜è´¨é‡ç­”æ¡ˆçš„ç´¢å¼•
    truncated_suspects = 0    # ç–‘ä¼¼è¢«æˆªæ–­çš„æ•°é‡

    with open(FILE_PATH, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if not line.strip(): continue
            try:
                data = json.loads(line)
                inst = data.get('instruction', data.get('question', ''))
                out = data.get('output', data.get('answer', ''))
                
                i_len = len(inst)
                o_len = len(out)
                
                instruction_lens.append(i_len)
                output_lens.append(o_len)
                
                # è´¨é‡æ£€æµ‹
                f_score, l_score = check_quality(out)
                if f_score and l_score:
                    high_quality_indices.append(i)
                
                # æˆªæ–­æ£€æµ‹ï¼šå¦‚æœç­”æ¡ˆé•¿åº¦éå¸¸æ¥è¿‘æœ€å¤§ Token é™åˆ¶ï¼Œä¸”ä¸ä»¥æ ‡ç‚¹ç»“æŸ
                if o_len > WARNING_LENGTH:
                    # ç®€å•æ£€æŸ¥æœ«å°¾æ ‡ç‚¹
                    if out.strip()[-1] not in ['ã€‚', '.', '!', '}', ']']:
                        truncated_suspects += 1
                        
            except:
                pass

    total = len(output_lens)
    if total == 0:
        print("âš ï¸ ç»“æœæ–‡ä»¶ä¸ºç©ºã€‚")
        return

    # ç»Ÿè®¡æ•°æ®
    avg_out = statistics.mean(output_lens)
    med_out = statistics.median(output_lens)
    max_out = max(output_lens)
    
    hq_count = len(high_quality_indices)
    hq_rate = (hq_count / total) * 100

    print(f"\nğŸ“ˆ === ç»“æœåˆ†å¸ƒæŠ¥å‘Š (åŸºæ•°: {total} æ¡) ===")
    print(f"âœ… æœ‰æ•ˆäº§å‡º: {total} æ¡")
    print(f"ğŸ’ é«˜è´¨é‡ç‡: {hq_rate:.1f}% (åŒæ—¶åŒ…å«å…¬å¼+é€»è¾‘è¯)")
    print(f"âœ‚ï¸ ç–‘ä¼¼æˆªæ–­: {truncated_suspects} æ¡ (å æ¯” {truncated_suspects/total*100:.1f}%)")
    print("-" * 30)
    
    print(f"ğŸ“ **Output (å›ç­”) é•¿åº¦ç»Ÿè®¡**:")
    print(f"   - å¹³å‡: {int(avg_out)} å­—")
    print(f"   - ä¸­ä½: {int(med_out)} å­—")
    print(f"   - æœ€é•¿: {max_out} å­—")
    
    # é•¿åº¦åˆ†å¸ƒç›´æ–¹å›¾
    print(f"\nğŸ“Š **å›ç­”é•¿åº¦åˆ†å¸ƒ (å¯»æ‰¾ MAX_OUTPUT_TOKENS ç”œèœœç‚¹)**:")
    bins = [0, 200, 500, 800, 1200, 2000, 5000]
    for k in range(len(bins)-1):
        low, high = bins[k], bins[k+1]
        count = sum(1 for l in output_lens if low <= l < high)
        bar = "â–ˆ" * int(count / total * 20)
        print(f"   [{low:4d}-{high:<4d} å­—]: {count:4d} | {bar} ({count/total*100:.1f}%)")

    print("-" * 30)
    
    # å»ºè®®éƒ¨åˆ†
    print("ğŸ’¡ **å‚æ•°è°ƒæ•´å»ºè®® (Sweet Spot)**:")
    
    # 1. å…³äº MAX_OUTPUT_TOKENS
    if truncated_suspects > total * 0.05:
        print(f"   ğŸ”´ **è­¦å‘Š**: æœ‰ >5% çš„å›ç­”å¯èƒ½è¢«æˆªæ–­äº†ï¼å»ºè®®è°ƒå¤§ `MAX_OUTPUT_TOKENS`ã€‚")
        print(f"      æ¨èå€¼: {int(max_out / 1.2)} tokens (æˆ–æ›´å¤§)")
    elif max_out < WARNING_LENGTH * 0.5:
        print(f"   ğŸŸ¢ **ç©ºé—´**: æ¨¡å‹å›ç­”éƒ½å¾ˆç²¾ç®€ã€‚ä½ å¯ä»¥è°ƒå° `MAX_OUTPUT_TOKENS` ä»¥ç¨å¾®æå‡å¹¶å‘é€Ÿåº¦ã€‚")
        print(f"      æ¨èå€¼: {int(max_out / 1.3)} tokens")
    else:
        print(f"   ğŸ”µ **å®Œç¾**: `MAX_OUTPUT_TOKENS` è®¾ç½®å¾—åˆšåˆšå¥½ï¼Œæ—¢æ²¡æˆªæ–­ä¹Ÿæ²¡æµªè´¹ã€‚")

    # 2. å…³äº MIN_TEXT_LENGTH (é€šè¿‡ output åæ¨)
    # è®¡ç®—é«˜è´¨é‡å›ç­”å¯¹åº”çš„å¹³å‡è¾“å…¥é•¿åº¦
    if hq_count > 0:
        hq_inst_lens = [instruction_lens[i] for i in high_quality_indices]
        avg_hq_inst = statistics.mean(hq_inst_lens)
        print(f"\n   ğŸŸ¡ **è¾“å…¥é™åˆ¶**: é«˜è´¨é‡å›ç­”(å«å…¬å¼)é€šå¸¸æ¥è‡ªé•¿åº¦çº¦ {int(avg_hq_inst)} å­—çš„é—®é¢˜ã€‚")
        print(f"      å»ºè®® `MIN_TEXT_LENGTH` ä¸è¦è¶…è¿‡ {int(avg_hq_inst * 0.5)}ï¼Œå¦åˆ™å¯èƒ½æ¼æ‰å¥½é—®é¢˜ã€‚")

if __name__ == "__main__":
    analyze_output_sweet_spot()