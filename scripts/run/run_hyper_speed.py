import json
import os
import time
import logging
import re
import asyncio
from openai import AsyncOpenAI
from tqdm.asyncio import tqdm

# ================= 1. âš¡ï¸ æé€Ÿé…ç½®åŒºåŸŸ =================
API_KEY = ""  # <--- ã€å¿…å¡«ã€‘
BASE_URL = ""    #apiè°ƒç”¨å¹³å°é“¾æ¥
MODEL_NAME = ""  #é€‰æ‹©ä½ çš„æ¨¡å‹

#ä»¥ä¸‹å»ºè®®æ ¹æ®sweetç»“æœè®¾ç½®
# ã€é…ç½® 1ã€‘å¹¶å‘æ•°ï¼š 50
CONCURRENCY = 50

# ã€é…ç½® 2ã€‘è¾“å…¥é™åˆ¶ï¼šæ”¾å®½ä¸‹é™ï¼Œæå‡ä¸Šé™ã€‚
MIN_TEXT_LENGTH = 100
MAX_TEXT_LENGTH = 3500 

# ã€é…ç½® 3ã€‘è¶…æ—¶æ–©æ€ï¼šå¾®è°ƒè‡³ 60ç§’
TIMEOUT_SECONDS = 60.0 

# ã€é…ç½® 4ã€‘æœ€å¤§ç”Ÿæˆé•¿åº¦
MAX_OUTPUT_TOKENS = 1280

# è·¯å¾„ä»¥åŠå‘½åè¯·åˆç†ä¿®æ”¹
WORK_DIR = r""
INPUT_FILE = os.path.join(WORK_DIR, "domain_chunks.jsonl")
OUTPUT_FILE = os.path.join(WORK_DIR, "sensor_physics_sft.jsonl")
LOG_FILE = os.path.join(WORK_DIR, "generation.log")

# ================= 2. Prompt  =================
#è¯·åˆç†ä¿®æ”¹
SYSTEM_PROMPT = r"""ä½ æ˜¯ä¸€ä½ä¼ æ„Ÿå™¨ææ–™ä¸å™¨ä»¶ç‰©ç†é¢†åŸŸçš„ä¸“å®¶ã€‚
è¯·åˆ†æç”¨æˆ·æä¾›çš„æ–‡çŒ®ç‰‡æ®µï¼Œæå–ç”¨äºå¾®è°ƒå¤§æ¨¡å‹çš„é—®ç­”å¯¹ã€‚

ã€æ ¸å¿ƒç›®æ ‡ï¼šç‰©ç†æœºç†ä¸æ•°å­¦è¡¨è¾¾å¹¶é‡ã€‘
1. **å¿…é¡»åŒ…å«æ€ç»´é“¾ (CoT)**ï¼š
   - Output å¿…é¡»å±•ç¤ºâ€œå¾®è§‚ç»“æ„ -> ç‰©ç†å‚æ•° -> å®è§‚æ€§èƒ½â€çš„æ¨å¯¼é€»è¾‘ã€‚
   - å¿…é¡»ä½¿ç”¨é€»è¾‘è¿æ¥è¯ï¼ˆå¦‚ï¼šå½’å› äºã€å¯¼è‡´ã€éµå¾ª...å®šå¾‹ã€å› æ­¤ï¼‰ã€‚

2. **ğŸš€ å¼ºåˆ¶æ•°å­¦åŒ–çº¦æŸ (å…³é”®)**ï¼š
   - å‡¡æ˜¯æ¶‰åŠç‰©ç†é‡ï¼ˆå¦‚çµæ•åº¦ã€ç”µå¯¼ç‡ã€æ´»åŒ–èƒ½ã€åŠ¿å’é«˜åº¦ï¼‰ï¼Œ**å¿…é¡»å°½å¯èƒ½è¡¥å……å¯¹åº”çš„æ•°å­¦è¡¨è¾¾**ï¼ˆä½¿ç”¨ LaTeX æ ¼å¼ï¼‰ã€‚
   - **å¦‚æœåŸæ–‡æ²¡æœ‰å…¬å¼ï¼Œè¯·æ ¹æ®ç‰©ç†å¸¸è¯†è¡¥å…¨åŸºç¡€å…¬å¼**ã€‚
   - ä¼ æ„Ÿå™¨é¢†åŸŸå¸¸ç”¨å…¬å¼ç¤ºä¾‹ï¼š
     * çµæ•åº¦ï¼š$S = R_a / R_g$ æˆ– $S = \Delta R / R_0$
     * å“åº”/æ¢å¤æ—¶é—´ï¼š$\tau_{res}$ (è¾¾åˆ° 90% å˜åŒ–æ‰€éœ€æ—¶é—´)
     * æ´»åŒ–èƒ½ï¼šArrhenius æ–¹ç¨‹ $k = A e^{-E_a/RT}$
     * å¸é™„æ¨¡å‹ï¼šLangmuir ç­‰æ¸©çº¿ $\theta = \frac{KP}{1+KP}$
     * ç”µé˜»å˜åŒ–ï¼š$R \propto e^{qV_b/kT}$

3. **LaTeX æ ¼å¼è¦æ±‚**ï¼š
   - JSONå­—ç¬¦ä¸²ä¸­çš„åæ–œæ å¿…é¡»è½¬ä¹‰ã€‚ä¾‹å¦‚ä½¿ç”¨ \\alpha è¡¨ç¤º \alphaã€‚

4. **æ ¼å¼ä¸æ•°é‡**ï¼š
   - è¾“å‡ºæ ‡å‡† JSONã€‚
   - æ ¹æ®è´¨é‡ç”Ÿæˆ 0-2 ä¸ªé—®ç­”å¯¹ã€‚æ— å®è´¨å†…å®¹è¿”å›ç©ºåˆ—è¡¨ã€‚

è¾“å‡ºç¤ºä¾‹ï¼š
{
  "qa_pairs": [
    {
      "instruction": "åˆ†æè¯¥ææ–™æ°”æ•æ€§èƒ½æå‡çš„åŸå› ã€‚",
      "output": "æ€§èƒ½æå‡ä¸»è¦å½’å› äºå¼‚è´¨ç»“çš„å½¢æˆã€‚æ ¹æ®è€—å°½å±‚ç†è®ºï¼Œå¼‚è´¨ç»“ç•Œé¢å¤„å½¢æˆäº†å†…å»ºç”µåœº..."
    }
  ]
}
"""

# ================= 3. å·¥å…·å‡½æ•° =================
def setup_logger(log_file_path):
    logger = logging.getLogger("AsyncQA")
    logger.setLevel(logging.INFO)
    if not logger.handlers:
        fh = logging.FileHandler(log_file_path, mode='a', encoding='utf-8') 
        fh.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
        logger.addHandler(fh)
    return logger

def fix_json_string(json_str):
    json_str = json_str.replace("```json", "").replace("```", "").strip()
    try:
        json_str = re.sub(r'(?<!\\)\\(?!["\\/bfnrtu])', r'\\\\', json_str)
    except Exception:
        pass
    return json_str

def count_lines(filename):
    if not os.path.exists(filename): return 0
    with open(filename, 'r', encoding='utf-8') as f:
        return sum(1 for _ in f)

# ================= 4. æ ¸å¿ƒé€»è¾‘ (æ–©æ€ç‰ˆ) =================
async def process_single_chunk(sem, client, text_chunk, chunk_id, logger):
    
    # è¿‡æ»¤å™¨ï¼šåªå¤„ç†é•¿åº¦é€‚ä¸­çš„æ–‡æœ¬
    if len(text_chunk) > MAX_TEXT_LENGTH:
        return chunk_id, text_chunk, None # å¤ªé•¿ä¸è¯»
    
    async with sem:
        # æé€Ÿç‰ˆä¸é‡è¯•ï¼šå¤±è´¥äº†å°±ç›´æ¥ä¸¢å¼ƒï¼Œä¸æµªè´¹æ—¶é—´é‡è¯•
        retries = 1 
        
        for attempt in range(retries):
            try:
                # ğŸ”ª æ–©æ€é€»è¾‘ï¼šasyncio.wait_for å¼ºåˆ¶è¶…æ—¶
                response = await asyncio.wait_for(
                    client.chat.completions.create(
                        model=MODEL_NAME,
                        messages=[
                            {"role": "system", "content": SYSTEM_PROMPT},
                            {"role": "user", "content": f"è¯·é˜…è¯»ä»¥ä¸‹ç§‘å­¦æ–‡çŒ®ç‰‡æ®µï¼Œå¹¶ç”Ÿæˆ0-2ä¸ªåŒ…å«ç‰©ç†çº¦æŸçš„é—®ç­”å¯¹ï¼š\n\n{text_chunk}"}
                        ],
                        temperature=0.3,
                        max_tokens=MAX_OUTPUT_TOKENS, # é™åˆ¶åºŸè¯
                        response_format={"type": "json_object"}
                    ),
                    timeout=TIMEOUT_SECONDS # è¶…è¿‡ç›´æ¥æ€
                )
                
                raw_content = response.choices[0].message.content
                cleaned_content = fix_json_string(raw_content)
                qa_data = json.loads(cleaned_content)
                return chunk_id, text_chunk, qa_data

            except asyncio.TimeoutError:
                # è®°å½•ä¸€ä¸‹è¢«æ€æ‰çš„ä»»åŠ¡ï¼ˆå¯é€‰ï¼‰
                # logger.warning(f"Chunk {chunk_id}: ğŸ”ª è¶…æ—¶æ–©æ€ ")
                return chunk_id, text_chunk, None
                
            except Exception as e:
                err_str = str(e)
                if "429" in err_str:
                    logger.warning(f"Chunk {chunk_id}: é™æµ 429ï¼Œé¿è®© 5ç§’...")
                    await asyncio.sleep(5)
                else:
                    # å…¶ä»–é”™è¯¯ç›´æ¥å¿½ç•¥ï¼Œä¸è®°å½•Errorä»¥å…åˆ·å±
                    pass
                
        return chunk_id, text_chunk, None

# ================= 5. ä¸»ç¨‹åº =================
async def main():
    logger = setup_logger(LOG_FILE)
    print(f"=== âš¡ï¸ æé€Ÿæ–©æ€ç‰ˆå¯åŠ¨ (å¹¶å‘: {CONCURRENCY}) ===")
    print(f"ç­–ç•¥: åªè¯» {MIN_TEXT_LENGTH}-{MAX_TEXT_LENGTH}å­— | è¶…æ—¶ {TIMEOUT_SECONDS}s å³æ€ | è¾“å‡ºé™ {MAX_OUTPUT_TOKENS} tokens")
    
    client = AsyncOpenAI(api_key=API_KEY, base_url=BASE_URL)
    
    # è¿›åº¦æ£€æŸ¥
    total_lines = count_lines(INPUT_FILE)
    processed_count = 0
    if os.path.exists(OUTPUT_FILE):
        processed_count = count_lines(OUTPUT_FILE)
        
    print(f"æ€»è¡Œæ•°: {total_lines} | å·²å­˜ç›˜: {processed_count}")
    
    # å¿«é€ŸåŠ è½½æ•°æ®
    print("æ­£åœ¨åŠ è½½æ•°æ®é˜Ÿåˆ—...")
    lines_to_process = []
    skipped = 0
    
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i < processed_count: continue
            
            # ç®€å•è¿‡æ»¤ï¼ŒåŠ é€ŸåŠ è½½
            if len(line) < MIN_TEXT_LENGTH: 
                skipped += 1
                continue
                
            try:
                data = json.loads(line)
                text = data.get('text', data.get('content', ''))
                
                # ä¸¥æ ¼çš„é•¿åº¦è¿‡æ»¤
                if len(text) < MIN_TEXT_LENGTH or len(text) > MAX_TEXT_LENGTH:
                    skipped += 1
                    continue
                    
                c_id = data.get('id', f"line_{i+1}")
                lines_to_process.append((c_id, text))
            except:
                pass

    print(f"æœ‰æ•ˆä»»åŠ¡: {len(lines_to_process)} æ¡ (å·²è¿‡æ»¤ä¸åˆæ ¼: {skipped} æ¡)")
    
    if not lines_to_process:
        print("æ— ä»»åŠ¡ã€‚")
        return

    sem = asyncio.Semaphore(CONCURRENCY)
    tasks = []

    for c_id, text in lines_to_process:
        tasks.append(process_single_chunk(sem, client, text, c_id, logger))
    
    # æ‰§è¡Œ
    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f_out:
        pbar = tqdm(asyncio.as_completed(tasks), total=len(tasks), desc="âš¡ï¸ Speed Run", unit="chk")
        
        valid_total = 0
        for future in pbar:
            try:
                chunk_id, origin_text, result = await future
                
                if result:
                    final_qas = result.get('qa_pairs', result) if isinstance(result, dict) else result
                    
                    if isinstance(final_qas, list) and len(final_qas) > 0:
                        valid_count = 0
                        for qa in final_qas:
                            q = qa.get("instruction", qa.get("question"))
                            a = qa.get("output", qa.get("answer"))
                            
                            if q and a:
                                record = {
                                    "source_chunk_id": chunk_id,
                                    "instruction": q,
                                    "output": a,
                                    "context_preview": origin_text[:50]
                                }
                                f_out.write(json.dumps(record, ensure_ascii=False) + "\n")
                                valid_count += 1
                        
                        if valid_count > 0:
                            f_out.flush()
                            valid_total += valid_count
                            logger.info(f"Chunk {chunk_id}: +{valid_count}")
                            pbar.set_postfix({"âœ… Saved": valid_total})
            except Exception:
                pass # æé€Ÿæ¨¡å¼ä¸‹å¿½ç•¥å†™å…¥é”™è¯¯ï¼Œä¿æŒå¥”è·‘

    print(f"\n=== å®Œæˆ ===")
    print(f"æ–°å¢æ•°æ®: {valid_total} æ¡")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·å¼ºåˆ¶åœæ­¢")